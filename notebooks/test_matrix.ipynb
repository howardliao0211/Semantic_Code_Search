{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382198ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False, False]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch, seq, emb = 3, 5, 10\n",
    "valid_len = torch.tensor(\n",
    "    [5, 3, 7],\n",
    ")\n",
    "X = torch.rand((batch, seq, emb))\n",
    "\n",
    "maxlen = X.size(-1)\n",
    "mask = torch.arange(maxlen, device=X.device)[None, :] < valid_len[:, None]\n",
    "mask = mask.unsqueeze(1).expand(-1, seq, -1)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 1\n",
    "\n",
    "subsequent_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d7377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 8192])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "batch_size = 64\n",
    "sequence_length = 128\n",
    "emb_size = 8192\n",
    "\n",
    "stop_size = 9\n",
    "\n",
    "pad_token = 2\n",
    "\n",
    "logits = torch.rand((batch_size, stop_size, emb_size))\n",
    "\n",
    "if logits.size(1) < sequence_length:\n",
    "    seq_to_pad = sequence_length - logits.size(1)\n",
    "    # Need to create a tensor with shape (64, to_pad, emb_size)\n",
    "    to_pad = torch.full(\n",
    "        size=(batch_size, seq_to_pad, emb_size),\n",
    "        fill_value=-1e9,\n",
    "        dtype=logits.dtype,\n",
    "        device=logits.device\n",
    "    )\n",
    "    logits = torch.cat((logits, to_pad), dim=1)\n",
    "\n",
    "logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
